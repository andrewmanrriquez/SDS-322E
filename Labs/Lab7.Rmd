---
title: "Lab 7"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, error = FALSE,
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print=100))

# Edit the file starting below
```

### Andrew Manrriquez

**This assignment is due by the end of the day. Only one student in the group submits a pdf file on Gradescope.**

*For all questions, include the R commands/functions that you used to find your answer (show R chunk). Answers without supporting code will not receive credit. Write full sentences to describe your findings.*

In this lab, you will explore one of the published novels of Jane Austen, accessible through the `janeaustenr` package. Let's first install it:

```{r, eval=FALSE}
# Install the janeaustenr package (Note, eval=FALSE means this code chunk is not
# submitted when knitting). You should run this command in the console ONCE. You
# do not need to run it again after the package is installed.
install.packages("janeaustenr")
```

Then load that package and other necessary packages for today:

```{r, warning=FALSE}
# Remember to install new packages with install.packages("name") in the console
# if you haven't already done that.
library(janeaustenr)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```

Let's take a quick look at the books available:

```{r}
# Take a quick look
austen_books() %>%
  group_by(book) %>%
  summarize(nb_lines = n())

dat <- austen_books()
```

The goal of the lab is to conduct some sentiment analysis for one of these books.

------------------------------------------------------------------------

### Question 1: (6 pts)

After calling all books with `austen_books()`, choose to keep **only one of the books**.

Then let's do some cleaning:

-   create a variable `chapter` with the following structure: `mutate(chapter = cumsum(str_detect(text, "")))`. Fill in the `""` with a regular expression to **find the lines mentioning the chapter sections**. *Hint: Take a look at how the chapter sections appear in your data first.*

-   get rid of the lines for chapter 0,

-   get rid of the empty lines,

-   get rid of the lines showing the chapter sections. *Hint: str_detect() with regex would be useful again!*. Note: some novels also have volumes. Get rid of those lines as well.

Save the resulting dataset as an object called `book`.

```{r}
# Choose the book "Persuasion"
book <- austen_books() %>%
  filter(book == "Persuasion") %>%
    mutate(chapter = cumsum(str_detect(text, "^Chapter\\s+[0-9]+"))) %>% 
  filter(!str_detect(text, "^Chapter [0-9]+")) %>% 
  filter(text != "") %>% 
  filter(chapter != 0)

```

How many chapters were contained in the `book` you chose?

```{r}
# Find all distinct chapters in the `book`
num_chapters <- book %>%
  summarise(num_chapters = n_distinct(chapter))
# Print number of chapters
print(num_chapters)
```

**There are 24 chapters in Persuasion.**

------------------------------------------------------------------------

### Question 2: (3 pts)

Next, we will split each line into words (this is sometimes called "tokenizatin"). One very convenient function to do this is `unnest_tokens(word, text)` (we do not need to specify the token as the default token is `words`). What are the 10 most common words in the book you chose? Do they reveal any pattern?

```{r}
# Tokenize the text into words and count occurrences
common_words <- book %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  top_n(10)

# Display the 10 most common words and their counts
common_words

```

**The top ten common words include: "the", "to", "and", "of", "a", "in", "was", "her", "had", "she". Based on these words the pattern appears to be that these words are articles and conjunctions.**

------------------------------------------------------------------------

### Question 3: (4 pts)

After getting the words by themselves, we will want to get rid of the stop words with the `SMART` lexicon:

```{r}
# Recall the SMART lexicon
SMARTstops <- stop_words %>% 
    filter(lexicon == "SMART")
```

Split each line in `book` into words with `unnest_tokens()` (like you did in Question 2) and use one of the joining function to get rid of stop words. Call the resulting dataset `words_books`.

```{r}
# Split each line into words and remove stop words
words_books <- book %>%
  unnest_tokens(word, text) %>%
  anti_join(SMARTstops)

# Display the first few rows of the resulting dataset
head(words_books)

```

Find the 10 most common words in `words_book` and display those in a word cloud. Do you notice any pattern in those words?

```{r}
# Load necessary libraries
library(wordcloud)

# Find the 10 most common words in words_book
top_words_book <- words_books %>%
  count(word, sort = TRUE) %>%
  top_n(10)

# Word cloud for the top words
wordcloud(words = top_words_book$word, freq = top_words_book$n,
          min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))

```

**Based on the word cloud the pattern in these words it that they are all nouns (names, gender, etc.) .**

------------------------------------------------------------------------

### Question 4: (5 pts)

Let's take a look at the sentiments associated with words in the book and how these sentiments change as the story goes. We will consider positive/negative words from the `sentiments` object:

```{r}
# Sentiments lexicon
head(sentiments)
```

Follow those steps to keep track of the sentiments as the story goes:

1.  Use a joining function to only keep the words in `words_book` that are associated with either a positive/negative sentiment.

2.  Find the number of words with positive and negative sentiment per chapter. *Hint: use group_by() with two variables.*

3.  Use a pivot function to have the number of positive words and negative words in separate columns.

4.  Find the proportion of words with a positive sentiment.

5.  Create a `ggplot` with `geom_line()` and `geom_smooth()` to represent the proportion of words with a positive sentiment across the chapters.

How do the sentiments evolve as the story goes?

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Sentiments lexicon
head(sentiments)

# Keep words associated with positive/negative sentiment
sentiment_words <- words_books %>%
  inner_join(sentiments, by = c(word = "word")) %>%
  filter(sentiment %in% c("positive", "negative"))

# Find the number of words with positive and negative sentiment per chapter
sentiment_counts <- sentiment_words %>%
  group_by(chapter, sentiment) %>%
  summarize(count = n())

# Pivot to have counts in separate columns for positive and negative sentiments
sentiment_counts_pivot <- sentiment_counts %>%
  spread(key = sentiment, value = count, fill = 0)

# Calculate the proportion of words with positive sentiment
sentiment_counts_pivot <- sentiment_counts_pivot %>%
  mutate(prop_positive = positive / (positive + negative))

# Create a ggplot for proportion of words with a positive sentiment
ggplot(sentiment_counts_pivot, aes(x = chapter, y = prop_positive)) +
  geom_line() +
  geom_smooth(method = "loess") +
  xlab("Chapter") +
  ylab("Proportion of Words with Positive Sentiment")

```

**From Chapter 1 to around Chapter 9 the sentiments become negative but at Chapter 10 the sentiments become positive until Chapter 15. After Chapter 15, the sentiments become negative and there are proportionally less words with positive sentiment at the end of the book, when compared to Chapter 1.**

------------------------------------------------------------------------

### Formatting: (2 pts)

Comment your code, write full sentences, and knit your file!
